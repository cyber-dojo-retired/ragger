
Exposing the red_amber_green.rb as a visible_file...
----------------------------------------------------
This would be good for people wanting to create their own LTF images.
However, it creates a tension because
- it can no longer be trusted to do an eval and call
- so it needs to be eval'd and called by the runner
- a runner call is much slower than a local eval.
However, is a half-way house?
- if the red_amber_green.rb file in the visible-files
  is the same as the one from the image's /usr/local/bin dir
  then it can be trusted.
- which means it can be eval'd and called locally.
- if it is NOT the same
  then eval and call it using the runner.

The problem with this is that puller could pull a new
LTF image with a new /usr/local/bin/red_amber_green.rb file...
and ragger will not see this and so it won't know to flush its cache.
This means resuming an old kata, with an old red_amber_green.rb,
may be slow, since it might never use a local eval and always
use the runner.
But cyber-dojo is not designed for long-lived katas...
Except that you can fork a new kata from a specific traffic-light...

We need ragger to know when puller pulls a new image.
Suppose each LTF image had its SHA inside it, eg as an env-var.
Now: puller.pull_image(image_name)
- docker pull image_name
- if new image pulled (see stdout), call ragger.new_image(image_name)

Now we have a potential multi-threading issue.
The cache could be accessed from one thread, and refreshed in another thread.
So... [gem install 'concurrent-ruby']
require 'concurrent'
class TrafficLight
  def initialize(external)
    @external = external
    @cache = Concurrent::Map.new
  end
  def colour(...)
    cached = @cache.compute_if_absent(image_name) {
      eval(get_rag_lambda_src(image_name, id), empty_binding)
    }
    rag = cached.call(stdout, stderr, status)
    ...
  end
  def new_image(image_name)
    @cache.delete(image_name)
  end
end
...
Now we will need to keep the source and the lambda in the cache
so we can compare the source against the visible_files version...
If there is a change, we must use the runner.
Otherwise, suppose there is an accidental infinite loop.
The eval would just spin till the invoking http-get timed out.

There is another problem...
puller,runner are daemonsets.
But ragger is currently a replicaCount:3
You can't rely on ragger being on the same node as puller.
Viz, the pullers on the 3 nodes could all end up notifying
the same ragger on one specific node. So two ragger's would
not get their cache notification.
Suppose ragger become a daemonset too.
That won't work either. Same reason.
puller and ragger could be side-car'd together.
Or, perhaps simpler, ragger contains the cron that does the puller.
Now, when puller detects a new image, it can notify ragger.
The cache would have enough complexity to warrant its own class I think.
The TrafficLight class does not want a new_image() method. The cache does.
Putting ragger and puller together means it will need to volume mount
/var/run/docker.sock
Cache::new_image(image_name) could now do one more thing...
It could get the rag-lambda source from the new image and then
compare it to the entry in the cache. Only if the source is
different does it need to update the cache entry. A lot of the time
a new image will have the same rag-lambda source.
This implies that even if the rag-lambda source is exposed as a
visible-file, a copy of it will still be needed inside the image.

At the moment puller is only deployed to beta.
This is because beta and prod share the same 3 nodes.
A puller+ragger service will need to
be deployed to both beta and prod. This makes the cron trickier.
It would be best if, eg, an env-var could be passed in
so on beta, the pull script new it was on beta and became a no-op.
k8s yaml... has env-vars for deployment.yaml
spec:
  containers:
  - name: ragger
    env:
    - name: CYBER_DOJO_K8S_DEPOLYMENT_NAMESPACE
      value: "beta"



require 'concurrent'
class RagLambdaCache
  def initialize(external)
    @external = external
    @cache = Concurrent::Map.new
  end
  def get(image_name, id)
    @cache[image_name] || new_image(image_name)
  end
  def new_image(image_name)
    files = { 'cyber-dojo.sh' => 'cat /usr/local/bin/red_amber_green.rb' }
    max_seconds = 1
    result = runner.run_cyber_dojo_sh(image_name, id, files, max_seconds)
    src = result['stdout']['content']
    rag = eval(src, empty_binding)
    @cache.compute(image_name, rag)
  end
  private
  def empty_binding
    binding
  end
  def runner
    @external.runner
  end
end

class Ragger
  def initialize(external, cache)
    @external = external
    @cache = cache
  end
  def sha
    ENV['SHA']
  end
  def alive?
    true
  end
  def ready?
    runner.ready?
  end
  def colour(image_name, id, stdout, stderr, status)
    rag = @cache.get(image_name, id).call(stdout, stderr, status)
    ...
  end
  def new_image(image_name)
    @cache.new_image(image_name)
  end
end

external = External.new
cache = RagLambdaCache.new(external)
ragger = Ragger.new(cache)

Now, the puller has to issue an http new_image() call to localhost.
So ragger needs to expose new_image(image_name)

- - - - - - - - - - - - - - - - - - - - - - - - - -
Another idea is if ragger could get a notification of a change to
an LTF image (done by puller) from dockerhub itself.
Is there such a notification?

However, there is still another potential problem.
Suppose there is a fault in the rag lambda and it results
in an infinite loop? This is only protected if the lambda
is run inside the runner.
But this would be handled by the lambda src being detected
as different to the master in the LTF image.




Initial design thoughts...
--------------------------
When a kata is created, if there is no red_amber_green.rb file,
get the red_amber_green.rb file from the image and add that as a visible_file.
Either way, red_amber_green.rb becomes a special file, like
cyber-dojo.sh and it can be edited but never deleted.
And it always appears in the lower filenames section.
